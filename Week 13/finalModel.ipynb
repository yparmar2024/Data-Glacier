{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488d4ec5",
   "metadata": {},
   "source": [
    "The code below imports the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec817c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch xgboost tqdm joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660b300",
   "metadata": {},
   "source": [
    "The code below loads the data and cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a862eca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleanTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit i cant use cause they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0   @user when a father is dysfunctional and is s...   \n",
       "1  @user @user thanks for #lyft credit i can't us...   \n",
       "2                                bihday your majesty   \n",
       "3  #model   i love u take with u all the time in ...   \n",
       "4             factsguide: society now    #motivation   \n",
       "\n",
       "                                          cleanTweet  \n",
       "0  when a father is dysfunctional and is so selfi...  \n",
       "1  thanks for lyft credit i cant use cause they d...  \n",
       "2                                bihday your majesty  \n",
       "3      model i love u take with u all the time in ur  \n",
       "4                  factsguide society now motivation  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random seeds for reporductibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "trainDf = pd.read_csv(\"train_E6oV3lV.csv\")\n",
    "testDf = pd.read_csv(\"test_tweets_anuFYb8.csv\")\n",
    "\n",
    "# Function for cleaning the data\n",
    "def cleanTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace(\"@user\", \"\")\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"[^a-z0-9\\s]\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Clean the data\n",
    "trainDf[\"cleanTweet\"] = trainDf[\"tweet\"].apply(cleanTweet)\n",
    "testDf[\"cleanTweet\"] = testDf[\"tweet\"].apply(cleanTweet)\n",
    "\n",
    "trainDf[[\"tweet\", \"cleanTweet\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c5a11",
   "metadata": {},
   "source": [
    "The code below implements the BERT form of featurization. We use BERT emebeddings since it understand contextual meaning, informal language and slang usage more efficiently than TF-IDF Vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f50e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 999/999 [08:29<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Final Embedding Matrix: (31962, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use MPS if available, fallback to CPU (Created for MacOS)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bertModel = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "bertModel.eval()\n",
    "\n",
    "# Use all tweets and labels from trainDf\n",
    "tweets = trainDf[\"cleanTweet\"].tolist()\n",
    "labels = trainDf[\"label\"].values\n",
    "\n",
    "# Custom dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, maxLen = 64):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxLen = maxLen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation = True,\n",
    "            padding = \"max_length\",\n",
    "            max_length = self.maxLen,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = TweetDataset(tweets, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "# Collect embeddings\n",
    "allEmbeddings = []\n",
    "\n",
    "# Generate [CLS] emebeddings batch-by-batch and store them\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc = \"Generating BERT Embeddings\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = bertModel(**batch)\n",
    "        clsEmbeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        allEmbeddings.append(clsEmbeddings)\n",
    "\n",
    "# Stack to final matrix\n",
    "Xbert = np.vstack(allEmbeddings)\n",
    "print(f\"Shape of Final Embedding Matrix: {Xbert.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf350a",
   "metadata": {},
   "source": [
    "The code below implements the Boosting Model family, or XGBoost, since the results were more accurate, specifically 95.4%, compared to the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a962b72",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Boosting Model: XGBoost\u001b[39;00m\n\u001b[1;32m     13\u001b[0m xgb \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     14\u001b[0m     n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     15\u001b[0m     max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myTrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myVal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msampleWeights\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m yPredXGB \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mpredict(Xval)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# EXTRA\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# Prepare target labels\n",
    "ybert = labels\n",
    "\n",
    "# Split into train and validation sets\n",
    "XTrain, Xval, yTrain, yVal = train_test_split(Xbert, ybert, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Calculate scale position weight parameter to balanced class weights\n",
    "# CHANGED FROM COUNTER TO SAMPLE WEIGHT\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sampleWeights = compute_sample_weight(class_weight = \"balanced\", y = yTrain)\n",
    "\n",
    "# Boosting Model: XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators = 300,\n",
    "    max_depth = 6,\n",
    "    learning_rate = 0.05,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    eval_metric = \"logloss\",\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "xgb.fit(XTrain, yTrain,\n",
    "        eval_set = [(Xval, yVal)],\n",
    "        early_stopping_rounds = 10,\n",
    "        verbose = True,\n",
    "        sample_weight = sampleWeights\n",
    ")\n",
    "\n",
    "yPredXGB = xgb.predict(Xval)\n",
    "\n",
    "# EXTRA\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "print(classification_report(yVal, yPredXGB, digits = 4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(yVal, yPredXGB))\n",
    "\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(yVal, yPredXGB)}\")\n",
    "print(classification_report(yVal, yPredXGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef81b9",
   "metadata": {},
   "source": [
    "The code below saves the model for future testing use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bb9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalModel.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model for future testing\n",
    "joblib.dump(xgb, \"finalModel.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ffe1d",
   "metadata": {},
   "source": [
    "The code below loads the model and tests it on the testing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e593d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Test Embeddings: 100%|██████████| 538/538 [05:11<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "xgbModel = joblib.load(\"finalModel.pkl\")\n",
    "\n",
    "# Tokenize the test tweets\n",
    "testTweets = testDf[\"cleanTweet\"].tolist()\n",
    "testDataset = TweetDataset(testTweets, tokenizer)\n",
    "testLoader = DataLoader(testDataset, batch_size = 32)\n",
    "\n",
    "# Generate BERT [CLS] embeddings for test data using modelBERT\n",
    "testEmbeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(testLoader, desc = \"Generating Test Embeddings\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = bertModel(**batch)\n",
    "        clsEmbeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        testEmbeddings.append(clsEmbeddings)\n",
    "\n",
    "XTestFinal = np.vstack(testEmbeddings)\n",
    "\n",
    "# Predict using the loaded classifier\n",
    "yPredTest = xgbModel.predict(XTestFinal)\n",
    "\n",
    "# Add predictions to the test dataframe\n",
    "testDf[\"predictedLabel\"] = yPredTest\n",
    "\n",
    "# Export the results to a CSV file\n",
    "testDf[[\"id\", \"predictedLabel\"]].to_csv(\"testPredictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b018d",
   "metadata": {},
   "source": [
    "The code below provides a function for the trained model to predict on user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcd3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I hate you and everything you stand for.\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Tweet: I love spending time with my family.\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Tweet: Go back to where you came from.\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Tweet: What a beautiful day it is today!\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Tweet: Kill all the terrorists.\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Tweet: Happy birthday! Hope you have a great day.\n",
      "Prediction: Non-Hate Speech\n",
      "\n",
      "Training set label counts: Counter({1: 23783, 0: 23783})\n",
      "Validation set label counts: Counter({0: 5937, 1: 456})\n"
     ]
    }
   ],
   "source": [
    "def predict(tweet, bertModel, xgbModel):\n",
    "    # Function for cleaning the data\n",
    "    def cleanTweet(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(\"@user\", \"\")\n",
    "        tweet = tweet.replace(\"#\", \"\")\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        tweet = re.sub(r\"[^a-z0-9\\s]\", \"\", tweet)\n",
    "        tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
    "        return tweet\n",
    "    \n",
    "    # Clean the tweet\n",
    "    cleanedTweet = cleanTweet(tweet)\n",
    "\n",
    "    # Tokenize and prepare input for BERT\n",
    "    encoding = tokenizer(\n",
    "        cleanedTweet,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = 64,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    # Generate BERT [CLS] embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = bertModel(**encoding)\n",
    "        clsEmbedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Use trained XGBoost model to predict\n",
    "    prediction = xgbModel.predict(clsEmbedding)\n",
    "\n",
    "    # Return predicted label\n",
    "    return prediction[0]\n",
    "\n",
    "def classifyTweet(tweet, bertModel, xgbModel):\n",
    "    labelMap = {0: \"Non-Hate Speech\", 1: \"Hate Speech\"}\n",
    "    print(f\"Tweet: {tweet}\\nPrediction: {labelMap[predict(tweet, bertModel, xgbModel)]}\\n\")\n",
    "\n",
    "\n",
    "classifyTweet(\"I hate you and everything you stand for.\", bertModel, xgbModel)\n",
    "classifyTweet(\"I love spending time with my family.\", bertModel, xgbModel)\n",
    "classifyTweet(\"Go back to where you came from.\", bertModel, xgbModel)\n",
    "classifyTweet(\"What a beautiful day it is today!\", bertModel, xgbModel)\n",
    "classifyTweet(\"Kill all the terrorists.\", bertModel, xgbModel)\n",
    "classifyTweet(\"Happy birthday! Hope you have a great day.\", bertModel, xgbModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
