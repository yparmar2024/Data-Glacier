{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488d4ec5",
   "metadata": {},
   "source": [
    "The code below imports the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec817c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: imblearn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch xgboost imblearn tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660b300",
   "metadata": {},
   "source": [
    "The code below loads the data and cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a862eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random seeds for reporductibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "trainDf = pd.read_csv(\"train_E6oV3lV.csv\")\n",
    "testDf = pd.read_csv(\"test_tweets_anuFYb8.csv\")\n",
    "\n",
    "# Function for cleaning the data\n",
    "def cleanTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace(\"@user\", \"\")\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"[^a-z0-9\\s]\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Clean the data\n",
    "trainDf[\"cleanTweet\"] = trainDf[\"tweet\"].apply(cleanTweet)\n",
    "testDf[\"cleanTweet\"] = testDf[\"tweet\"].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c5a11",
   "metadata": {},
   "source": [
    "The code below implements the BERT form of featurization. We use BERT emebeddings since it understand contextual meaning, informal language and slang usage more efficiently than TF-IDF Vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f50e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 1998/1998 [07:51<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Final Embedding Matrix: torch.Size([31962, 768])\n",
      "Embeddings saved to BERTEmbeddings.pt\n"
     ]
    }
   ],
   "source": [
    "# Use MPS for faster embeddings generated\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dataset class for BERT embeddings\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, maxLen = 64):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxLen = maxLen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation = True,\n",
    "            padding = \"max_length\",\n",
    "            max_length = self.maxLen,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "def generateEmbeddings(tweets, tokenizer, model, device, path = \"BERTEmbeddings.pt\"):\n",
    "    print(\"Generating embeddings...\")\n",
    "\n",
    "    # Instantiate dataset and dataloader\n",
    "    dataset = TweetDataset(tweets, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size = 16)\n",
    "                        \n",
    "    # Collect embeddings\n",
    "    allEmbeddings = []\n",
    "\n",
    "    # Generate [CLS] emebeddings batch-by-batch and store them\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc = \"Generating BERT Embeddings\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            clsEmbeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "            allEmbeddings.append(clsEmbeddings)\n",
    "\n",
    "    # Concatenate to final matrix\n",
    "    Xbert = torch.cat(allEmbeddings)\n",
    "    print(f\"Shape of Final Embedding Matrix: {Xbert.shape}\")\n",
    "\n",
    "    # Save to cache\n",
    "    torch.save(Xbert, path)\n",
    "    print(f\"Embeddings saved to {path}\")\n",
    "\n",
    "    return Xbert.cpu().numpy()\n",
    "\n",
    "# Generate embeddings for train set\n",
    "tweets = trainDf[\"cleanTweet\"].tolist()\n",
    "Xbert = generateEmbeddings(tweets, tokenizer, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf350a",
   "metadata": {},
   "source": [
    "The code below implements the Multilayer Perceptron (MLP) deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a962b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced MLP Deep Learning Model - SMOTE Balanced\n",
      "\n",
      "Training Enhanced MLP Deep Learning Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 744/744 [00:08<00:00, 86.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.248, Accuracy = 0.8167, F1Hate = 0.4099, F1NonHate = 0.8915, F1Macro = 0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 744/744 [00:04<00:00, 161.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.145, Accuracy = 0.8911, F1Hate = 0.5146, F1NonHate = 0.9387, F1Macro = 0.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 744/744 [00:04<00:00, 176.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.106, Accuracy = 0.9280, F1Hate = 0.5915, F1NonHate = 0.9605, F1Macro = 0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 744/744 [00:04<00:00, 179.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.082, Accuracy = 0.9349, F1Hate = 0.6183, F1NonHate = 0.9644, F1Macro = 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 744/744 [00:04<00:00, 177.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.069, Accuracy = 0.9384, F1Hate = 0.6137, F1NonHate = 0.9665, F1Macro = 0.7901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 744/744 [00:04<00:00, 177.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.058, Accuracy = 0.9313, F1Hate = 0.6034, F1NonHate = 0.9624, F1Macro = 0.7829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 744/744 [00:04<00:00, 176.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.049, Accuracy = 0.9520, F1Hate = 0.6341, F1NonHate = 0.9743, F1Macro = 0.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 744/744 [00:06<00:00, 107.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.043, Accuracy = 0.9504, F1Hate = 0.6369, F1NonHate = 0.9734, F1Macro = 0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 744/744 [00:06<00:00, 114.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.039, Accuracy = 0.9451, F1Hate = 0.6317, F1NonHate = 0.9703, F1Macro = 0.8010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 744/744 [00:07<00:00, 94.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.034, Accuracy = 0.9484, F1Hate = 0.6452, F1NonHate = 0.9722, F1Macro = 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 744/744 [00:09<00:00, 78.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss = 0.034, Accuracy = 0.9506, F1Hate = 0.6417, F1NonHate = 0.9735, F1Macro = 0.8076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 744/744 [00:04<00:00, 163.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss = 0.028, Accuracy = 0.9456, F1Hate = 0.6321, F1NonHate = 0.9706, F1Macro = 0.8014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 744/744 [00:04<00:00, 166.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss = 0.026, Accuracy = 0.9499, F1Hate = 0.6322, F1NonHate = 0.9731, F1Macro = 0.8027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 744/744 [00:04<00:00, 182.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss = 0.027, Accuracy = 0.9402, F1Hate = 0.6134, F1NonHate = 0.9676, F1Macro = 0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 744/744 [00:04<00:00, 178.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 0.015, Accuracy = 0.9493, F1Hate = 0.6400, F1NonHate = 0.9727, F1Macro = 0.8064\n",
      "Early stopping at Epoch 15\n",
      "\n",
      "Loaded Best Model with Hate F1 Score: 0.6452 at Epoch 10\n",
      "Optimal Threshold for Hate Speech Detection: 0.200\n",
      "\n",
      "Final Results with Optimized Threshold\n",
      "\n",
      "Final Accuracy: 0.9476\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.97      0.97      0.97      5937\n",
      "        Hate       0.63      0.65      0.64       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.80      0.81      0.81      6393\n",
      "weighted avg       0.95      0.95      0.95      6393\n",
      "\n",
      "Final Confusion Matrix:\n",
      "[[5760  177]\n",
      " [ 158  298]]\n",
      "\n",
      "Final F1 Scores:\n",
      "    Hate Speech F1: 0.6402\n",
      "    Non-Hate Speech F1: 0.9717\n",
      "    Macro F1: 0.8060\n",
      "\n",
      "Best Model saved to: enhancedMLPModel.pth\n",
      "To load this model later, use:\n",
      "    checkpoint = torch.load(\"enhancedMLPModel.pth\")\n",
      "    model = MLP(**checkpoint['model_architecture'])\n",
      "    model.load_state_dict(checkpoint['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# Prepare target labels\n",
    "ybert = trainDf[\"label\"].values\n",
    "\n",
    "# Split train/validation\n",
    "XTrain, XVal, yTrain, yVal = train_test_split(Xbert, ybert, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Balance training set skewed towards non-hate speech labels\n",
    "smote = SMOTE(random_state = 42, k_neighbors = 3)\n",
    "XTrainSMOTE, yTrainSMOTE = smote.fit_resample(XTrain, yTrain)\n",
    "\n",
    "# Change device to CPU since MPS freezes on last few batches\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create a Enhanced MLP Deep Learning Model\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, inputDim = 768, hiddenDims = [512, 256, 128], numClasses = 2, dropoutRate = 0.4):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "\n",
    "        # Build multiple hidden layers\n",
    "        layers = []\n",
    "        prevDim = inputDim\n",
    "\n",
    "        for i, hiddenDim in enumerate(hiddenDims):\n",
    "            layers.append(nn.Linear(prevDim, hiddenDim))\n",
    "            layers.append(nn.BatchNorm1d(hiddenDim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropoutRate))\n",
    "            prevDim = hiddenDim\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prevDim, numClasses))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train the MLP Model\n",
    "def trainEnhancedMLP(XTrainSMOTE, yTrainSMOTE, XVal, yVal):\n",
    "    # Set device to CPU, as MPS freezes at last couple of batches\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    XTrainTensor = torch.from_numpy(XTrainSMOTE.astype(np.float32))\n",
    "    yTrainTensor = torch.from_numpy(yTrainSMOTE.astype(np.int64))\n",
    "    XValTensor = torch.from_numpy(XVal.astype(np.float32))\n",
    "    yValTensor = torch.from_numpy(yVal.astype(np.int64))\n",
    "\n",
    "    # Create data loaders\n",
    "    trainDataset = TensorDataset(XTrainTensor, yTrainTensor)\n",
    "    trainLoader = DataLoader(trainDataset, batch_size = 64, shuffle = True, num_workers = 0, pin_memory = False)\n",
    "\n",
    "    # Initialize MLP Deep Learning Model\n",
    "    model = EnhancedMLP(\n",
    "        inputDim = 768,\n",
    "        hiddenDims = [512, 256, 128],\n",
    "        numClasses = 2,\n",
    "        dropoutRate = 0.4\n",
    "    ).to(device)\n",
    "\n",
    "    # Use weighted loss to handle class imbalance\n",
    "    classWeights = compute_class_weight(\"balanced\", classes = np.unique(yTrainSMOTE), y = yTrainSMOTE)\n",
    "    classWeightsTensor = torch.tensor(classWeights, dtype = torch.float32).to(device)\n",
    "\n",
    "    # Use weighted CrossEntropyLoss\n",
    "    criterion = nn.CrossEntropyLoss(weight = classWeightsTensor)\n",
    "\n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"max\", factor = 0.5, patience = 3)\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 20\n",
    "    bestHateF1 = 0.0\n",
    "    bestModelState = None\n",
    "    patience = 5\n",
    "    patienceCounter = 0\n",
    "\n",
    "    print(\"Training Enhanced MLP Deep Learning Model...\")\n",
    "\n",
    "    # Training Phase with Early Stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        totalLoss = 0.0\n",
    "\n",
    "        for batchX, batchY in tqdm(trainLoader, desc = f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            batchX, batchY = batchX.to(device), batchY.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batchX)\n",
    "            loss = criterion(outputs, batchY)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            totalLoss += loss.item()\n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valOutputs = model(XValTensor.to(device))\n",
    "            valProbs = torch.softmax(valOutputs, dim = 1)\n",
    "\n",
    "            # Use optimal threshold for hate speech detection, lower threshold favors hate speech detection\n",
    "            threshold = 0.3\n",
    "            valPreds = (valProbs[:, 1] > threshold).long()\n",
    "            valPredsNP = valPreds.cpu().numpy()\n",
    "\n",
    "        # Calculate F1 Scores\n",
    "        F1Macro = f1_score(yVal, valPredsNP, average = \"macro\")\n",
    "        F1Hate = f1_score(yVal, valPredsNP, pos_label = 1)\n",
    "        F1NonHate = f1_score(yVal, valPredsNP, pos_label = 0)\n",
    "        accuracy = (valPredsNP == yVal).mean()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {totalLoss/len(trainLoader):.3f}, \"\n",
    "            f\"Accuracy = {accuracy:.4f}, F1Hate = {F1Hate:.4f}, F1NonHate = {F1NonHate:.4f}, \"\n",
    "            f\"F1Macro = {F1Macro:.4f}\")\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(F1Hate)\n",
    "\n",
    "        # Track best model based on hate F1 score\n",
    "        if F1Hate > bestHateF1:\n",
    "            bestHateF1 = F1Hate\n",
    "            bestHateF1Epoch = epoch\n",
    "            bestModelState = model.state_dict().copy()\n",
    "            patienceCounter = 0\n",
    "        else:\n",
    "            patienceCounter += 1\n",
    "\n",
    "        if patienceCounter >= patience:\n",
    "            print(f\"Early stopping at Epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model for final evaluation\n",
    "    if bestModelState is not None:\n",
    "        model.load_state_dict(bestModelState)\n",
    "        print(f\"\\nLoaded Best Model with Hate F1 Score: {bestHateF1:.4f} at Epoch {bestHateF1Epoch + 1}\")\n",
    "\n",
    "    # Final Evaluation with Threshold Tuning\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        finalOutputs = model(XValTensor.to(device))\n",
    "        finalProbs = torch.softmax(finalOutputs, dim = 1)\n",
    "\n",
    "        # Test different thresholds to find optimal one\n",
    "        thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "        bestThreshold = 0.5\n",
    "        bestF1 = 0.0\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            preds = (finalProbs[:, 1] > threshold).long().cpu().numpy()\n",
    "            f1 = f1_score(yVal, preds, pos_label = 1)\n",
    "            if f1 > bestF1:\n",
    "                bestF1 = f1\n",
    "                bestThreshold = threshold\n",
    "        \n",
    "        print(f\"Optimal Threshold for Hate Speech Detection: {bestThreshold:.3f}\")\n",
    "\n",
    "        # Final prediction using optimal threshold\n",
    "        finalPreds = (finalProbs[:, 1] > bestThreshold).long()\n",
    "        finalPredsNP = finalPreds.cpu().numpy()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nFinal Results with Optimized Threshold\\n\")\n",
    "\n",
    "    finalAccuracy = (finalPredsNP == yVal).mean()\n",
    "    print(f\"Final Accuracy: {finalAccuracy:.4f}\")\n",
    "\n",
    "    print(\"Final Classification Report:\")\n",
    "    print(classification_report(yVal, finalPredsNP, target_names = [\"Non-Hate\", \"Hate\"]))\n",
    "\n",
    "    print(\"Final Confusion Matrix:\")\n",
    "    print(confusion_matrix(yVal, finalPredsNP))\n",
    "\n",
    "    F1HateFinal = f1_score(yVal, finalPredsNP, pos_label = 1)\n",
    "    F1NonHateFinal = f1_score(yVal, finalPredsNP, pos_label = 0)\n",
    "    F1MacroFinal = f1_score(yVal, finalPredsNP, average = \"macro\")\n",
    "\n",
    "    print(f\"\\nFinal F1 Scores:\")\n",
    "    print(f\"    Hate Speech F1: {F1HateFinal:.4f}\")\n",
    "    print(f\"    Non-Hate Speech F1: {F1NonHateFinal:.4f}\")\n",
    "    print(f\"    Macro F1: {F1MacroFinal:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    modelSavePath = f\"enhancedMLPModel.pth\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_architecture\": {\n",
    "            \"inputDim\": 768,\n",
    "            \"hiddenDims\": [512, 256, 128],\n",
    "            \"numClasses\": 2,\n",
    "            \"dropoutRate\": 0.4\n",
    "        },\n",
    "        \"best_hate_f1\": bestHateF1,\n",
    "        \"optimal_threshold\": bestThreshold,\n",
    "        \"final_metrics\": {\n",
    "            \"accuracy\": finalAccuracy,\n",
    "            \"hate_f1\": F1HateFinal,\n",
    "            \"non_hate_f1\": F1NonHateFinal,\n",
    "            \"macro_f1\": F1MacroFinal\n",
    "        }\n",
    "    }, modelSavePath)\n",
    "\n",
    "    print(f\"\\nBest Model saved to: {modelSavePath}\")\n",
    "    print(f\"To load this model later, use:\")\n",
    "    print(f\"    checkpoint = torch.load(\\\"{modelSavePath}\\\")\")\n",
    "    print(f\"    model = MLP(**checkpoint['model_architecture'])\")\n",
    "    print(f\"    model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "\n",
    "    return model, finalPredsNP, bestThreshold\n",
    "\n",
    "print(\"Enhanced MLP Deep Learning Model - SMOTE Balanced\\n\")\n",
    "enhancedModel, enhancedPreds, optimalThreshold = trainEnhancedMLP(XTrainSMOTE, yTrainSMOTE, XVal, yVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ffe1d",
   "metadata": {},
   "source": [
    "The code below loads the model and tests it on the testing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e593d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 1075/1075 [12:55<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Final Embedding Matrix: torch.Size([17197, 768])\n",
      "Embeddings saved to testBERTEmbeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/6b7wzf7x14x25j715qn6rth00000gn/T/ipykernel_67593/1142247259.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"enhancedMLPModel.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions completed!\n",
      "Predictions saved to 'testPredictions.csv'\n",
      "Total predictions: 17197\n",
      "Hate speech predictions: 1329\n",
      "Non-hate speech predictions: 15868\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for test set\n",
    "testTweets = testDf[\"cleanTweet\"].tolist()\n",
    "XTestBert = generateEmbeddings(testTweets, tokenizer, model.to(device), device, \"testBERTEmbeddings.pt\")\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint = torch.load(\"enhancedMLPModel.pth\")\n",
    "testModel = EnhancedMLP(**checkpoint[\"model_architecture\"])\n",
    "testModel.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "testModel.eval()\n",
    "\n",
    "# Get optimal threshold from saved model\n",
    "optimalThreshold = checkpoint[\"optimal_threshold\"]\n",
    "\n",
    "# Make predictions on test set\n",
    "device = torch.device(\"cpu\")\n",
    "testModel.to(device)\n",
    "\n",
    "XTestTensor = torch.from_numpy(XTestBert.astype(np.float32)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    testOutputs = testModel(XTestTensor)\n",
    "    testProbs = torch.softmax(testOutputs, dim = 1)\n",
    "    testPreds = (testProbs[:, 1] > optimalThreshold).long().cpu().numpy()\n",
    "\n",
    "# Create predictions file\n",
    "testDf[\"label\"] = testPreds\n",
    "predictions = testDf[[\"id\", \"label\"]].copy()\n",
    "predictions.to_csv(\"testPredictions.csv\", index = False)\n",
    "\n",
    "print(f\"Test predictions completed!\")\n",
    "print(f\"Predictions saved to 'testPredictions.csv'\")\n",
    "print(f\"Total predictions: {len(testPreds)}\")\n",
    "print(f\"Hate speech predictions: {sum(testPreds)}\")\n",
    "print(f\"Non-hate speech predictions: {len(testPreds) - sum(testPreds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b018d",
   "metadata": {},
   "source": [
    "The code below provides a function for the trained model to predict on user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbcd3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/6b7wzf7x14x25j715qn6rth00000gn/T/ipykernel_67593/799524318.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"enhancedMLPModel.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-Hate Speech Statement: \n",
      "original_text: I love your approach to life, it's very unique.\n",
      "cleaned_text: i love your approach to life its very unique\n",
      "prediction: 0\n",
      "label: Non-Hate Speech\n",
      "confidence: 1.3302735624165507e-06\n",
      "threshold_used: 0.20000000000000004\n",
      "\n",
      "Hate Speech Statement: \n",
      "original_text: You are such a stupid idiot.\n",
      "cleaned_text: you are such a stupid idiot\n",
      "prediction: 1\n",
      "label: Hate Speech\n",
      "confidence: 0.997912585735321\n",
      "threshold_used: 0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Function to predict text hate speech\n",
    "def predictHateSpeech(text, threshold = None):\n",
    "    # Load model and get optimal threshold\n",
    "    checkpoint = torch.load(\"enhancedMLPModel.pth\")\n",
    "    model = EnhancedMLP(**checkpoint[\"model_architecture\"])\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    if threshold is None:\n",
    "        threshold = checkpoint[\"optimal_threshold\"]\n",
    "    \n",
    "    # Clean the input text\n",
    "    cleanedText = cleanTweet(text)\n",
    "    \n",
    "    # Generate BERT embedding for the text\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load BERT model for embedding\n",
    "    bertModel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bertModel.eval().to(device)\n",
    "    \n",
    "    # Tokenize and encode\n",
    "    encoding = tokenizer(\n",
    "        cleanedText,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = 64,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = bertModel(**encoding.to(device))\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Make prediction\n",
    "    embeddingTensor = torch.from_numpy(embedding.astype(np.float32)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(embeddingTensor)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        hateProb = probs[0, 1].item()\n",
    "        prediction = 1 if hateProb > threshold else 0\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        \"original_text\": text,\n",
    "        \"cleaned_text\": cleanedText,\n",
    "        \"prediction\": prediction,\n",
    "        \"label\": \"Hate Speech\" if prediction == 1 else \"Non-Hate Speech\",\n",
    "        \"confidence\": hateProb,\n",
    "        \"threshold_used\": threshold\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example Non-Hate Speech Prediction\n",
    "result = predictHateSpeech(\"I love your approach to life, it's very unique.\")\n",
    "print(\"\\nNon-Hate Speech Statement: \")\n",
    "for row in result:\n",
    "    print(f\"{row}: {result[row]}\")\n",
    "\n",
    "# Example Hate Speech Prediction\n",
    "result = predictHateSpeech(\"You are such a stupid idiot.\")\n",
    "print(\"\\nHate Speech Statement: \")\n",
    "for row in result:\n",
    "    print(f\"{row}: {result[row]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
