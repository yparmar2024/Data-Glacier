{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a077a0",
   "metadata": {},
   "source": [
    "The code below imports the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e40732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch xgboost tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2f54f",
   "metadata": {},
   "source": [
    "The code below loads the data and cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01450642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleanTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit i cant use cause they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0   @user when a father is dysfunctional and is s...   \n",
       "1  @user @user thanks for #lyft credit i can't us...   \n",
       "2                                bihday your majesty   \n",
       "3  #model   i love u take with u all the time in ...   \n",
       "4             factsguide: society now    #motivation   \n",
       "\n",
       "                                          cleanTweet  \n",
       "0  when a father is dysfunctional and is so selfi...  \n",
       "1  thanks for lyft credit i cant use cause they d...  \n",
       "2                                bihday your majesty  \n",
       "3      model i love u take with u all the time in ur  \n",
       "4                  factsguide society now motivation  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random seeds for reporductibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "trainDf = pd.read_csv(\"train_E6oV3lV.csv\")\n",
    "testDf = pd.read_csv(\"test_tweets_anuFYb8.csv\")\n",
    "\n",
    "# Function for cleaning the data\n",
    "def cleanTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace(\"@user\", \"\")\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"[^a-z0-9\\s]\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Clean the data\n",
    "trainDf[\"cleanTweet\"] = trainDf[\"tweet\"].apply(cleanTweet)\n",
    "testDf[\"cleanTweet\"] = testDf[\"tweet\"].apply(cleanTweet)\n",
    "\n",
    "trainDf[[\"tweet\", \"cleanTweet\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b10d5",
   "metadata": {},
   "source": [
    "The code below implements the BERT form of featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653f340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 999/999 [09:38<00:00,  1.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Final Embedding Matrix: (31962, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use MPS if available, fallback to CPU (Created for MacOS)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Use all tweets and labels from trainDf\n",
    "tweets = trainDf[\"cleanTweet\"].tolist()\n",
    "labels = trainDf[\"label\"].values\n",
    "\n",
    "# Custom dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, maxLen = 64):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxLen = maxLen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation = True,\n",
    "            padding = \"max_length\",\n",
    "            max_length = self.maxLen,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = TweetDataset(tweets, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "# Collect embeddings\n",
    "allEmbeddings = []\n",
    "\n",
    "# Generate [CLS] emebeddings batch-by-batch and store them\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc = \"Generating BERT Embeddings\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        clsEmbeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        allEmbeddings.append(clsEmbeddings)\n",
    "\n",
    "# Stack to final matrix\n",
    "Xbert = np.vstack(allEmbeddings)\n",
    "print(f\"Shape of Final Embedding Matrix: {Xbert.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47018369",
   "metadata": {},
   "source": [
    "The code below implements the Linear Model family, Ensemble Model family, and Boosting Model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5534c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.87533239480682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      5937\n",
      "           1       0.34      0.81      0.48       456\n",
      "\n",
      "    accuracy                           0.88      6393\n",
      "   macro avg       0.66      0.84      0.70      6393\n",
      "weighted avg       0.94      0.88      0.90      6393\n",
      "\n",
      "Random Forest Accuracy: 0.9486938839355545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      5937\n",
      "           1       0.93      0.30      0.46       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.94      0.65      0.72      6393\n",
      "weighted avg       0.95      0.95      0.94      6393\n",
      "\n",
      "XGBoost Accuracy: 0.9540122008446739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      5937\n",
      "           1       0.75      0.53      0.62       456\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.86      0.76      0.80      6393\n",
      "weighted avg       0.95      0.95      0.95      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare target labels\n",
    "ybert = labels\n",
    "\n",
    "# Split into train and validation sets\n",
    "XTrain, Xval, yTrain, yVal = train_test_split(Xbert, ybert, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Linear Model: Logistic Regression\n",
    "logReg = LogisticRegression(max_iter = 1000, class_weight = \"balanced\")\n",
    "logReg.fit(XTrain, yTrain)\n",
    "yPredLogReg = logReg.predict(Xval)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(yVal, yPredLogReg)}\")\n",
    "print(classification_report(yVal, yPredLogReg))\n",
    "\n",
    "# Ensemble Model: Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = \"balanced\")\n",
    "rf.fit(XTrain, yTrain)\n",
    "yPredRF = rf.predict(Xval)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(yVal, yPredRF)}\")\n",
    "print(classification_report(yVal, yPredRF))\n",
    "\n",
    "# Calculate scale position weight parameter to balanced class weights\n",
    "counter = Counter(yTrain)\n",
    "scalePosWeight = counter[0] / counter[1]\n",
    "\n",
    "# Boosting Model: XGBoost\n",
    "xgb = XGBClassifier(eval_metric = \"logloss\", random_state = 42, scale_pos_weight = scalePosWeight)\n",
    "xgb.fit(XTrain, yTrain)\n",
    "yPredXGB = xgb.predict(Xval)\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(yVal, yPredXGB)}\")\n",
    "print(classification_report(yVal, yPredXGB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
