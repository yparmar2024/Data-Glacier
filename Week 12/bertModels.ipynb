{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a077a0",
   "metadata": {},
   "source": [
    "The code below imports the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e40732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: imblearn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch xgboost imblearn tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2f54f",
   "metadata": {},
   "source": [
    "The code below loads the data and cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01450642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random seeds for reporductibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "trainDf = pd.read_csv(\"train_E6oV3lV.csv\")\n",
    "testDf = pd.read_csv(\"test_tweets_anuFYb8.csv\")\n",
    "\n",
    "# Function for cleaning the data\n",
    "def cleanTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace(\"@user\", \"\")\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"[^a-z0-9\\s]\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# Clean the data\n",
    "trainDf[\"cleanTweet\"] = trainDf[\"tweet\"].apply(cleanTweet)\n",
    "testDf[\"cleanTweet\"] = testDf[\"tweet\"].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b10d5",
   "metadata": {},
   "source": [
    "The code below implements the BERT form of featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653f340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 1998/1998 [08:34<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Final Embedding Matrix: torch.Size([31962, 768])\n",
      "Embeddings saved to BERTEmbeddings.pt\n"
     ]
    }
   ],
   "source": [
    "# Use MPS for faster embeddings generated\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dataset class for BERT embeddings\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, maxLen = 64):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxLen = maxLen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation = True,\n",
    "            padding = \"max_length\",\n",
    "            max_length = self.maxLen,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "def generateEmbeddings(tweets, tokenizer, model, device, path = \"BERTEmbeddings.pt\"):\n",
    "    print(\"Generating embeddings...\")\n",
    "\n",
    "    # Instantiate dataset and dataloader\n",
    "    dataset = TweetDataset(tweets, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size = 16)\n",
    "                        \n",
    "    # Collect embeddings\n",
    "    allEmbeddings = []\n",
    "\n",
    "    # Generate [CLS] emebeddings batch-by-batch and store them\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc = \"Generating BERT Embeddings\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            clsEmbeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "            allEmbeddings.append(clsEmbeddings)\n",
    "\n",
    "    # Concatenate to final matrix\n",
    "    Xbert = torch.cat(allEmbeddings)\n",
    "    print(f\"Shape of Final Embedding Matrix: {Xbert.shape}\")\n",
    "\n",
    "    # Save to cache\n",
    "    torch.save(Xbert, path)\n",
    "    print(f\"Embeddings saved to {path}\")\n",
    "\n",
    "    return Xbert.cpu().numpy()\n",
    "\n",
    "# Generate embeddings for train set\n",
    "tweets = trainDf[\"cleanTweet\"].tolist()\n",
    "Xbert = generateEmbeddings(tweets, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47018369",
   "metadata": {},
   "source": [
    "The code below implements the Linear Model family, Ensemble Model family, and Boosting Model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5534c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Linear Model Family - Logistic Regression\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logisitic Regression Training: 100%|██████████| 1/1 [00:33<00:00, 33.45s/it]\n",
      "Logistic Regression Prediction): 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.98      0.90      0.94      5937\n",
      "        hate       0.38      0.77      0.51       456\n",
      "\n",
      "    accuracy                           0.89      6393\n",
      "   macro avg       0.68      0.84      0.72      6393\n",
      "weighted avg       0.94      0.89      0.91      6393\n",
      "\n",
      "\n",
      "2. Ensemble Model Family - Random Forest\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Forest Training: 100%|██████████| 1/1 [00:16<00:00, 16.28s/it]\n",
      "Random Forest Prediction: 100%|██████████| 1/1 [00:00<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.97      0.94      0.96      5937\n",
      "        hate       0.46      0.68      0.55       456\n",
      "\n",
      "    accuracy                           0.92      6393\n",
      "   macro avg       0.72      0.81      0.75      6393\n",
      "weighted avg       0.94      0.92      0.93      6393\n",
      "\n",
      "\n",
      "3. Ensemble Model Family - XGBoost\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoost Training: 100%|██████████| 1/1 [00:06<00:00,  6.46s/it]\n",
      "XGBoost Prediction: 100%|██████████| 1/1 [00:00<00:00, 55.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.98      0.92      0.95      5937\n",
      "        hate       0.40      0.73      0.52       456\n",
      "\n",
      "    accuracy                           0.90      6393\n",
      "   macro avg       0.69      0.82      0.73      6393\n",
      "weighted avg       0.94      0.90      0.91      6393\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare target labels\n",
    "ybert = trainDf[\"label\"].values\n",
    "\n",
    "# Split train/validation\n",
    "XTrain, XVal, yTrain, yVal = train_test_split(Xbert, ybert, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Balance training set skewed towards non-hate speech labels\n",
    "smote = SMOTE(random_state = 42, k_neighbors = 3)\n",
    "XTrainSMOTE, yTrainSMOTE = smote.fit_resample(XTrain, yTrain)\n",
    "\n",
    "# Linear Model: Logistic Regression\n",
    "print(\"\\n1. Linear Model Family - Logistic Regression\\n\")\n",
    "logReg = LogisticRegression(\n",
    "    max_iter = 5000,\n",
    "    solver = \"liblinear\",\n",
    "    C = 1.0,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "with tqdm(total = 1, desc = \"Logisitic Regression Training\") as pbar:\n",
    "    logReg.fit(XTrainSMOTE, yTrainSMOTE)\n",
    "    pbar.update(1)\n",
    "\n",
    "with tqdm(total = 1, desc = \"Logistic Regression Prediction)\") as pbar:\n",
    "    yPredLogReg = logReg.predict(XVal)\n",
    "    pbar.update(1)\n",
    "    \n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(yVal, yPredLogReg):.4f}\")\n",
    "print(classification_report(yVal, yPredLogReg, target_names = [\"non-hate\", \"hate\"]))\n",
    "\n",
    "# Ensemble Model: Random Forest\n",
    "print(\"\\n2. Ensemble Model Family - Random Forest\\n\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators = 50,\n",
    "    max_depth = 10,\n",
    "    min_samples_split = 5,\n",
    "    min_samples_leaf = 2,\n",
    "    n_jobs = -1,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "with tqdm(total = 1, desc = \"Random Forest Training\") as pbar:\n",
    "    rf.fit(XTrainSMOTE, yTrainSMOTE)\n",
    "    pbar.update(1)\n",
    "\n",
    "with tqdm(total = 1, desc = \"Random Forest Prediction\") as pbar:\n",
    "    yPredRF = rf.predict(XVal)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(yVal, yPredRF):.4f}\")\n",
    "print(classification_report(yVal, yPredRF, target_names = [\"non-hate\", \"hate\"]))\n",
    "\n",
    "# Boosting Model: XGBoost\n",
    "print(\"\\n3. Ensemble Model Family - XGBoost\\n\")\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators = 50,\n",
    "    max_depth = 4,\n",
    "    learning_rate = 0.2,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    eval_metric = \"logloss\",\n",
    "    random_state = 42,\n",
    "    n_jobs = -1,\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "with tqdm(total = 1, desc = \"XGBoost Training\") as pbar:\n",
    "    xgb.fit(XTrainSMOTE, yTrainSMOTE)\n",
    "    pbar.update(1)\n",
    "\n",
    "with tqdm(total = 1, desc = \"XGBoost Prediction\") as pbar:\n",
    "    yPredXGB = xgb.predict(XVal)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(yVal, yPredXGB):.4f}\")\n",
    "print(classification_report(yVal, yPredXGB, target_names = [\"non-hate\", \"hate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf8432",
   "metadata": {},
   "source": [
    "The code below implements the MLP Deep Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f9cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Deep Learning Model - SMOTE Balanced\n",
      "\n",
      "Training MLP Deep Learning Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1487/1487 [00:04<00:00, 355.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.275, Accuracy = 0.9024, F1Hate = 0.5378, F1NonHate = 0.9454, F1Macro = 0.7416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1487/1487 [00:04<00:00, 358.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.197, Accuracy = 0.8889, F1Hate = 0.5130, F1NonHate = 0.9373, F1Macro = 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1487/1487 [00:04<00:00, 341.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.146, Accuracy = 0.9199, F1Hate = 0.5733, F1NonHate = 0.9558, F1Macro = 0.7646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1487/1487 [00:04<00:00, 359.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.114, Accuracy = 0.9341, F1Hate = 0.5987, F1NonHate = 0.9641, F1Macro = 0.7814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1487/1487 [00:04<00:00, 327.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.091, Accuracy = 0.9360, F1Hate = 0.6064, F1NonHate = 0.9652, F1Macro = 0.7858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1487/1487 [00:04<00:00, 366.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.072, Accuracy = 0.9366, F1Hate = 0.5962, F1NonHate = 0.9656, F1Macro = 0.7809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1487/1487 [00:04<00:00, 352.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.058, Accuracy = 0.9377, F1Hate = 0.6113, F1NonHate = 0.9662, F1Macro = 0.7887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1487/1487 [00:04<00:00, 361.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.049, Accuracy = 0.9432, F1Hate = 0.6033, F1NonHate = 0.9694, F1Macro = 0.7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1487/1487 [00:04<00:00, 312.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.043, Accuracy = 0.9496, F1Hate = 0.6265, F1NonHate = 0.9730, F1Macro = 0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1487/1487 [00:04<00:00, 345.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.039, Accuracy = 0.9376, F1Hate = 0.6014, F1NonHate = 0.9661, F1Macro = 0.7838\n",
      "\n",
      "Loaded Best Model with Hate F1 Score: 0.6265 at Epoch 9\n",
      "\n",
      "Final Results after 10 Epochs\n",
      "\n",
      "Final Accuracy: 0.9376\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.97      0.96      0.97      5937\n",
      "        Hate       0.55      0.66      0.60       456\n",
      "\n",
      "    accuracy                           0.94      6393\n",
      "   macro avg       0.76      0.81      0.78      6393\n",
      "weighted avg       0.94      0.94      0.94      6393\n",
      "\n",
      "Final Confusion Matrix:\n",
      "[[5693  244]\n",
      " [ 155  301]]\n",
      "\n",
      "Final F1 Scores:\n",
      "    Hate Speech F1: 0.6014\n",
      "    Non-Hate Speech F1: 0.9661\n",
      "    Macro F1: 0.7838\n",
      "\n",
      "Best Model saved to: bestMLPModel.pth\n",
      "To load this model later, use:\n",
      "    checkpoint = torch.load(\"bestMLPModel.pth\")\n",
      "    model = MLP(**checkpoint['model_architecture'])\n",
      "    model.load_state_dict(checkpoint['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# Change device to CPU since MPS freezes on last few batches\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create MLP Deep Learning Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputDim = 768, hiddenDim = 256, numClasses = 2, dropoutRate = 0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputDim, hiddenDim)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.fc2 = nn.Linear(hiddenDim, numClasses)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the MLP Model\n",
    "def trainMLP(XTrainSMOTE, yTrainSMOTE, XVal, yVal):\n",
    "    # Set device to CPU, as MPS freezes at last couple of batches\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    XTrainTensor = torch.from_numpy(XTrainSMOTE.astype(np.float32))\n",
    "    yTrainTensor = torch.from_numpy(yTrainSMOTE.astype(np.int64))\n",
    "    XValTensor = torch.from_numpy(XVal.astype(np.float32))\n",
    "    yValTensor = torch.from_numpy(yVal.astype(np.int64))\n",
    "\n",
    "    # Create data loaders\n",
    "    trainDataset = TensorDataset(XTrainTensor, yTrainTensor)\n",
    "    trainLoader = DataLoader(trainDataset, batch_size = 32, shuffle = True, num_workers = 0, pin_memory = False)\n",
    "\n",
    "    # Initialize MLP Deep Learning Model\n",
    "    model = MLP().to(device)\n",
    "\n",
    "    # Use weighted loss to handle class imbalance\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 10\n",
    "    bestHateF1 = 0.0\n",
    "    bestModelState = None\n",
    "\n",
    "    print(\"Training MLP Deep Learning Model...\")\n",
    "\n",
    "    # Training Phase\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        totalLoss = 0.0\n",
    "\n",
    "        for batchX, batchY in tqdm(trainLoader, desc = f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            batchX, batchY = batchX.to(device), batchY.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batchX)\n",
    "            loss = criterion(outputs, batchY)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            totalLoss += loss.item()\n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valOutputs = model(XValTensor.to(device))\n",
    "            _, valPreds = torch.max(valOutputs, 1)\n",
    "            valPredsNP = valPreds.cpu().numpy()\n",
    "\n",
    "        # Calculate F1 Scores\n",
    "        F1Macro = f1_score(yVal, valPredsNP, average = \"macro\")\n",
    "        F1Hate = f1_score(yVal, valPredsNP, pos_label = 1)\n",
    "        F1NonHate = f1_score(yVal, valPredsNP, pos_label = 0)\n",
    "\n",
    "        accuracy = (valPredsNP == yVal).mean()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {totalLoss/len(trainLoader):.3f}, \"\n",
    "            f\"Accuracy = {accuracy:.4f}, F1Hate = {F1Hate:.4f}, F1NonHate = {F1NonHate:.4f}, \"\n",
    "            f\"F1Macro = {F1Macro:.4f}\")\n",
    "\n",
    "        # Track best model based on hate F1 score\n",
    "        if F1Hate > bestHateF1:\n",
    "            bestHateF1 = F1Hate\n",
    "            bestHateF1Epoch = epoch\n",
    "            bestModelState = model.state_dict().copy()\n",
    "\n",
    "    # Load best model for final evaluation\n",
    "    if bestModelState is not None:\n",
    "        model.load_state_dict(bestModelState)\n",
    "        print(f\"\\nLoaded Best Model with Hate F1 Score: {bestHateF1:.4f} at Epoch {bestHateF1Epoch + 1}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        finalOutputs = model(XValTensor.to(device))\n",
    "        _, finalPreds = torch.max(finalOutputs, 1)\n",
    "        finalPredsNP = finalPreds.cpu().numpy()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nFinal Results after {epochs} Epochs\\n\")\n",
    "\n",
    "    finalAccuracy = (finalPredsNP == yVal).mean()\n",
    "    print(f\"Final Accuracy: {finalAccuracy:.4f}\")\n",
    "\n",
    "    print(\"Final Classification Report:\")\n",
    "    print(classification_report(yVal, finalPredsNP, target_names = [\"Non-Hate\", \"Hate\"]))\n",
    "\n",
    "    print(\"Final Confusion Matrix:\")\n",
    "    print(confusion_matrix(yVal, finalPredsNP))\n",
    "\n",
    "    F1HateFinal = f1_score(yVal, finalPredsNP, pos_label = 1)\n",
    "    F1NonHateFinal = f1_score(yVal, finalPredsNP, pos_label = 0)\n",
    "    F1MacroFinal = f1_score(yVal, finalPredsNP, average = \"macro\")\n",
    "\n",
    "    print(f\"\\nFinal F1 Scores:\")\n",
    "    print(f\"    Hate Speech F1: {F1HateFinal:.4f}\")\n",
    "    print(f\"    Non-Hate Speech F1: {F1NonHateFinal:.4f}\")\n",
    "    print(f\"    Macro F1: {F1MacroFinal:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    modelSavePath = f\"bestMLPModel.pth\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_architecture\": {\n",
    "            \"inputDim\": 768,\n",
    "            \"hiddenDim\": 256,\n",
    "            \"numClasses\": 2,\n",
    "            \"dropoutRate\": 0.3\n",
    "        },\n",
    "        \"best_hate_f1\": bestHateF1,\n",
    "        \"final_metrics\": {\n",
    "            \"accuracy\": finalAccuracy,\n",
    "            \"hate_f1\": F1HateFinal,\n",
    "            \"non_hate_f1\": F1NonHateFinal,\n",
    "            \"macro_f1\": F1MacroFinal\n",
    "        }\n",
    "    }, modelSavePath)\n",
    "\n",
    "    print(f\"\\nBest Model saved to: {modelSavePath}\")\n",
    "    print(f\"To load this model later, use:\")\n",
    "    print(f\"    checkpoint = torch.load(\\\"{modelSavePath}\\\")\")\n",
    "    print(f\"    model = MLP(**checkpoint['model_architecture'])\")\n",
    "    print(f\"    model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "\n",
    "    return model, finalPredsNP\n",
    "\n",
    "print(\"MLP Deep Learning Model - SMOTE Balanced\\n\")\n",
    "model, preds = trainMLP(XTrainSMOTE, yTrainSMOTE, XVal, yVal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
